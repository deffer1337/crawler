# crawler
Начальные требования:
* не использовать scrapy/готовые библиотеки для краулинга
* парсить можно html.parser либо использовать более продвинутые библиотечки типа beautifulsoap/lxml
* Сохранение страниц на диск
* не скачивать повторно
* Переход по ссылкам в рамках указанных доменов [текущего, группы, etc]
* поддержка robots.txt
* Скачивание в несколько потоков
* Возможность докачки